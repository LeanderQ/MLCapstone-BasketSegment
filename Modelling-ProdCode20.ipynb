{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "##########################################\n",
    "# This notebook is used for intial data exploration for the capstone project\n",
    "##########################################\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we import the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Before DropNA', 250546)\n",
      "('After DropNA', 175047)\n",
      "('After Uninteresting categories', 122605)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import glob\n",
    "from IPython.display import display # Allows the use of display() for DataFrames\n",
    "\n",
    "\n",
    "data = pd.read_csv('data/transactions_200607.csv')  #import one file\n",
    "\n",
    "#import all csvs (courtesy of http://stackoverflow.com/questions/20906474/import-multiple-csv-files-into-pandas-and-concatenate-into-one-dataframe)\n",
    "\n",
    "path =r'data' # use your path\n",
    "allFiles = glob.glob(path + \"/tr*.csv\")\n",
    "\n",
    "#print allFiles\n",
    "#data = pd.DataFrame()\n",
    "#list_ = []\n",
    "#for file_ in allFiles:\n",
    "#    df = pd.read_csv(file_,index_col=None, header=0)\n",
    "#    list_.append(df)\n",
    "#data = pd.concat(list_)\n",
    "\n",
    "#print(data.head(20))\n",
    "\n",
    "\n",
    "print('Before DropNA',len(data))\n",
    "data=data.dropna()\n",
    "print('After DropNA',len(data))\n",
    "\n",
    "data = data[data.CUST_PRICE_SENSITIVITY != 'XX']\n",
    "data = data[data.CUST_LIFESTAGE != 'OT']\n",
    "print('After Uninteresting categories',len(data))\n",
    "#list(data.columns.values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reformat the data into a wide dataset using Prod_code_20 as the lowest level of aggregation.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PROD_CODE_20     DEP00001  DEP00002  DEP00003  DEP00004  DEP00005  DEP00006  \\\n",
      "BASKET_ID                                                                     \n",
      "994100100152667      0.00       0.0      0.00      0.00       0.0       0.0   \n",
      "994100100152709      0.00       0.0      0.98      0.00       0.0       0.0   \n",
      "994100100152710      1.03       0.0      0.00      0.00       0.0       0.0   \n",
      "994100100152771      0.00       0.0      1.01      0.00       0.0       0.0   \n",
      "994100100152772      2.25       0.0      0.00      0.56       0.0       0.0   \n",
      "\n",
      "PROD_CODE_20     DEP00007  DEP00008  DEP00009  DEP00010  \\\n",
      "BASKET_ID                                                 \n",
      "994100100152667       0.0      0.00       0.0       0.0   \n",
      "994100100152709       0.0      1.17       0.0       0.0   \n",
      "994100100152710       0.0      0.00       0.0       0.0   \n",
      "994100100152771       0.0      0.00       0.0       0.0   \n",
      "994100100152772       0.0      1.46       0.0       0.8   \n",
      "\n",
      "PROD_CODE_20              ...            SPEND  SHOP_WEEKDAY  SHOP_HOUR  \\\n",
      "BASKET_ID                 ...                                             \n",
      "994100100152667           ...             2.58             1         14   \n",
      "994100100152709           ...            26.39             2         15   \n",
      "994100100152710           ...             1.03             7         13   \n",
      "994100100152771           ...             4.17             7         15   \n",
      "994100100152772           ...            22.94             3          8   \n",
      "\n",
      "PROD_CODE_20     BASKET_SIZE  BASKET_TYPE  BASKET_DOMINANT_MISSION  \\\n",
      "BASKET_ID                                                            \n",
      "994100100152667            M   Small Shop                  Grocery   \n",
      "994100100152709            L    Full Shop                    Mixed   \n",
      "994100100152710            S   Small Shop                    Fresh   \n",
      "994100100152771            M       Top Up                    Fresh   \n",
      "994100100152772            L    Full Shop                    Mixed   \n",
      "\n",
      "PROD_CODE_20     STORE_FORMAT  STORE_REGION  CUST_LIFESTAGE  \\\n",
      "BASKET_ID                                                     \n",
      "994100100152667            LS           W01              OA   \n",
      "994100100152709            LS           N03              OA   \n",
      "994100100152710            LS           N03              OA   \n",
      "994100100152771            LS           S01              OA   \n",
      "994100100152772            LS           N03              OA   \n",
      "\n",
      "PROD_CODE_20     CUST_PRICE_SENSITIVITY  \n",
      "BASKET_ID                                \n",
      "994100100152667                      UM  \n",
      "994100100152709                      MM  \n",
      "994100100152710                      MM  \n",
      "994100100152771                      LA  \n",
      "994100100152772                      LA  \n",
      "\n",
      "[5 rows x 99 columns]\n",
      "PROD_CODE_20     DEP00001  DEP00002  DEP00003  DEP00004  DEP00005  DEP00006  \\\n",
      "BASKET_ID                                                                     \n",
      "994100100152667      0.00       0.0      0.00      0.00       0.0       0.0   \n",
      "994100100152709      0.00       0.0      0.98      0.00       0.0       0.0   \n",
      "994100100152710      1.03       0.0      0.00      0.00       0.0       0.0   \n",
      "994100100152771      0.00       0.0      1.01      0.00       0.0       0.0   \n",
      "994100100152772      2.25       0.0      0.00      0.56       0.0       0.0   \n",
      "\n",
      "PROD_CODE_20     DEP00007  DEP00008  DEP00009  DEP00010  \\\n",
      "BASKET_ID                                                 \n",
      "994100100152667       0.0      0.00       0.0       0.0   \n",
      "994100100152709       0.0      1.17       0.0       0.0   \n",
      "994100100152710       0.0      0.00       0.0       0.0   \n",
      "994100100152771       0.0      0.00       0.0       0.0   \n",
      "994100100152772       0.0      1.46       0.0       0.8   \n",
      "\n",
      "PROD_CODE_20              ...            STORE_REGION_N02  STORE_REGION_N03  \\\n",
      "BASKET_ID                 ...                                                 \n",
      "994100100152667           ...                         0.0               0.0   \n",
      "994100100152709           ...                         0.0               1.0   \n",
      "994100100152710           ...                         0.0               1.0   \n",
      "994100100152771           ...                         0.0               0.0   \n",
      "994100100152772           ...                         0.0               1.0   \n",
      "\n",
      "PROD_CODE_20     STORE_REGION_S01  STORE_REGION_S02  STORE_REGION_S03  \\\n",
      "BASKET_ID                                                               \n",
      "994100100152667               0.0               0.0               0.0   \n",
      "994100100152709               0.0               0.0               0.0   \n",
      "994100100152710               0.0               0.0               0.0   \n",
      "994100100152771               1.0               0.0               0.0   \n",
      "994100100152772               0.0               0.0               0.0   \n",
      "\n",
      "PROD_CODE_20     STORE_REGION_W01  STORE_REGION_W02  STORE_REGION_W03  \\\n",
      "BASKET_ID                                                               \n",
      "994100100152667               1.0               0.0               0.0   \n",
      "994100100152709               0.0               0.0               0.0   \n",
      "994100100152710               0.0               0.0               0.0   \n",
      "994100100152771               0.0               0.0               0.0   \n",
      "994100100152772               0.0               0.0               0.0   \n",
      "\n",
      "PROD_CODE_20     CUST_LIFESTAGE  CUST_PRICE_SENSITIVITY  \n",
      "BASKET_ID                                                \n",
      "994100100152667              OA                      UM  \n",
      "994100100152709              OA                      MM  \n",
      "994100100152710              OA                      MM  \n",
      "994100100152771              OA                      LA  \n",
      "994100100152772              OA                      LA  \n",
      "\n",
      "[5 rows x 122 columns]\n",
      "18118\n"
     ]
    }
   ],
   "source": [
    "#create basketprofiles.  Simplest version will be to sum spend in each category in PROD_CODE_20, keeping the target variable as well\n",
    "#pivot code from http://stackoverflow.com/questions/41046766/using-and-graphing-the-results-of-a-crosstab-dataframe-in-python\n",
    "\n",
    "data_cross=data.pivot_table(index='BASKET_ID', columns='PROD_CODE_20', values='SPEND', aggfunc=np.sum, fill_value=0)\n",
    "data_cross.reset_index(level=['BASKET_ID'], inplace=True)\n",
    "\n",
    "#group the variables that are unique to each basket\n",
    "bybasket=data.groupby(['BASKET_ID'])\n",
    "targetsByBasket=pd.DataFrame(bybasket['CUST_LIFESTAGE', 'CUST_PRICE_SENSITIVITY'].first())\n",
    "targetsByBasket.reset_index(level=['BASKET_ID'], inplace=True)\n",
    "\n",
    "colsByBasketInt=pd.DataFrame(bybasket['SHOP_WEEKDAY', 'SHOP_HOUR'].first())\n",
    "colsByBasketInt.reset_index(level=['BASKET_ID'], inplace=True)\n",
    "\n",
    "colsByBasketStr=pd.DataFrame(bybasket['BASKET_SIZE', 'BASKET_TYPE','BASKET_DOMINANT_MISSION','STORE_FORMAT','STORE_REGION'].first())\n",
    "colsByBasketStr.reset_index(level=['BASKET_ID'], inplace=True)\n",
    "colsByBasketStr_dummied = pd.get_dummies(colsByBasketStr, prefix=['BASKET_SIZE', 'BASKET_TYPE','BASKET_DOMINANT_MISSION','STORE_FORMAT','STORE_REGION'])\n",
    "\n",
    "sumsByBasket=pd.DataFrame(bybasket['SPEND'].sum())\n",
    "sumsByBasket.reset_index(level=['BASKET_ID'], inplace=True)\n",
    "\n",
    "data_cross_dummied = pd.merge(data_cross, sumsByBasket, how='inner', on = 'BASKET_ID')\n",
    "data_cross_dummied = pd.merge(data_cross_dummied, colsByBasketInt, how='inner', on = 'BASKET_ID')\n",
    "data_cross_dummied = pd.merge(data_cross_dummied, colsByBasketStr_dummied, how='inner', on = 'BASKET_ID')\n",
    "data_cross_dummied = pd.merge(data_cross_dummied, targetsByBasket, how='inner', on = 'BASKET_ID')\n",
    "\n",
    "\n",
    "data_cross = pd.merge(data_cross, sumsByBasket, how='inner', on = 'BASKET_ID')\n",
    "data_cross = pd.merge(data_cross, colsByBasketInt, how='inner', on = 'BASKET_ID')\n",
    "data_cross = pd.merge(data_cross, colsByBasketStr, how='inner', on = 'BASKET_ID')\n",
    "data_cross = pd.merge(data_cross, targetsByBasket, how='inner', on = 'BASKET_ID')\n",
    "\n",
    "\n",
    "#reset the index to what it should be for the rest\n",
    "data_cross.set_index(['BASKET_ID'], inplace=True)\n",
    "data_cross_dummied.set_index(['BASKET_ID'], inplace=True)\n",
    "\n",
    "print(data_cross.head(5))\n",
    "print(data_cross_dummied.head(5))\n",
    "print(len(data_cross_dummied))\n",
    "#list(data_cross.columns.values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Naive Classifier Work"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#do all the work for the naive classifier\n",
    "\n",
    "#create variables for the naive classifier\n",
    "spendByBasket=pd.DataFrame(bybasket['SPEND'].sum())\n",
    "spendByBasket.reset_index(level=['BASKET_ID'], inplace=True)\n",
    "\n",
    "basketTypeByBasket=pd.DataFrame(bybasket['BASKET_TYPE'].first())\n",
    "basketTypeByBasket.reset_index(level=['BASKET_ID'], inplace=True)\n",
    "\n",
    "basketSizeByBasket=pd.DataFrame(bybasket['BASKET_SIZE'].first())\n",
    "basketSizeByBasket.reset_index(level=['BASKET_ID'], inplace=True)\n",
    "\n",
    "weekdayByBasket=pd.DataFrame(bybasket['SHOP_WEEKDAY'].first())\n",
    "weekdayByBasket.reset_index(level=['BASKET_ID'], inplace=True)\n",
    "\n",
    "#merge them back in\n",
    "data_cross_naive = pd.merge(data_cross, spendByBasket, how='inner', on = 'BASKET_ID')\n",
    "data_cross_naive = pd.merge(data_cross_naive, basketTypeByBasket, how='inner', on = 'BASKET_ID')\n",
    "data_cross_naive = pd.merge(data_cross_naive, basketSizeByBasket, how='inner', on = 'BASKET_ID')\n",
    "data_cross_naive = pd.merge(data_cross_naive, weekdayByBasket, how='inner', on = 'BASKET_ID')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#create the naive predictor\n",
    "\n",
    "avgTrainSpend=X_train['SPEND'].mean()\n",
    "stDevTrainSpend=X_train['SPEND'].std()\n",
    "\n",
    "\n",
    "def NaiveClassifier(test, trainMean, trainStd):\n",
    "\n",
    "    if(test['SPEND']<trainMean-trainStd):\n",
    "            if(test['BASKET_TYPE']='Full Shop')\n",
    "                   naivePriceSens = 'LA'\n",
    "            else:\n",
    "                    naivePriceSens = 'MM'\n",
    "    elif(test['SPEND']>rainMean-trainStd):\n",
    "        if(test['BASKET_TYPE']='Full Shop')\n",
    "                   naivePriceSens = 'MM\n",
    "            else:\n",
    "                    naivePriceSens = 'UM'\n",
    "    else:\n",
    "        naivePriceSens = 'MM'\n",
    "    \n",
    "    if (test[\"SHOP_WEEKDAY\"] = 7) or (test[\"SHOP_WEEKDAY\"] = 1):\n",
    "        if(test['BASKET_SIZE'] = 'Small'):\n",
    "            if(test['BASKET_TYPE']='Small Shop')\n",
    "                   naivePriceSens = 'YA'\n",
    "            else:\n",
    "                    naivePriceSens = 'OA'\n",
    "        elif(test['BASKET_SIZE'] = 'Medium'):\n",
    "            naiveLifeStage = 'YF'\n",
    "        else:\n",
    "            naiveLifeStage = 'OF'            \n",
    "    else:\n",
    "        naiveLifeStage = 'PE'\n",
    "    \n",
    "    \n",
    "    \n",
    "    return naivePriceSens + \"-\" naiveLifeStage\n",
    "\n",
    "#apply to every row\n",
    "#X_test.apply(NaiveClassifier, args=(avgTrainSpend,stDevTrainSpend)) # returns DataFrame\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#pop the two classifiers used for the naive predictor before doing any classification\n",
    "X_train.pop(\"SPEND\")\n",
    "X_train.pop(\"SHOP_WEEKDAY\")\n",
    "X_test.pop(\"SPEND\")\n",
    "X_test.pop(\"SHOP_WEEKDAY\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Split the data #should probably consider stratified sampling here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "120\n",
      "120\n",
      "2\n",
      "2\n"
     ]
    }
   ],
   "source": [
    "#create the training, testing split.  can't use sklearn.cross_validation.train_test_split since I have two targets\n",
    "\n",
    "#first do a 70-30 split.  \n",
    "train_X=data_cross.sample(frac=0.7,random_state=42)\n",
    "test_X=data_cross.drop(train_X.index)\n",
    "\n",
    "#pop off the classifiers\n",
    "train_y = train_X[[\"CUST_LIFESTAGE\",\"CUST_PRICE_SENSITIVITY\"]] #potentially for use in multi-output decision trees\n",
    "train_y_LS = train_X.pop(\"CUST_LIFESTAGE\")\n",
    "train_y_PS = train_X.pop(\"CUST_PRICE_SENSITIVITY\")\n",
    "\n",
    "test_y = test_X[[\"CUST_LIFESTAGE\",\"CUST_PRICE_SENSITIVITY\"]] #potentially for use in multi-output decision trees\n",
    "test_y_LS = test_X.pop(\"CUST_LIFESTAGE\")\n",
    "test_y_PS = test_X.pop(\"CUST_PRICE_SENSITIVITY\")\n",
    "\n",
    "#repeate for dummied data\n",
    "#first do a 70-30 split.  \n",
    "train_X_dummied=data_cross_dummied.sample(frac=0.7,random_state=42)\n",
    "test_X_dummied=data_cross_dummied.drop(train_X_dummied.index)\n",
    "\n",
    "#pop off the classifiers\n",
    "train_y_dummied = train_X_dummied[[\"CUST_LIFESTAGE\",\"CUST_PRICE_SENSITIVITY\"]] #potentially for use in multi-output decision trees\n",
    "train_y_LS_dummied = train_X_dummied.pop(\"CUST_LIFESTAGE\")\n",
    "train_y_PS_dummied = train_X_dummied.pop(\"CUST_PRICE_SENSITIVITY\")\n",
    "\n",
    "test_y_dummied = test_X_dummied[[\"CUST_LIFESTAGE\",\"CUST_PRICE_SENSITIVITY\"]] #potentially for use in multi-output decision trees\n",
    "test_y_LS_dummied = test_X_dummied.pop(\"CUST_LIFESTAGE\")\n",
    "test_y_PS_dummied = test_X_dummied.pop(\"CUST_PRICE_SENSITIVITY\")\n",
    "\n",
    "print(len(train_X_dummied.columns))\n",
    "print(len(test_X_dummied.columns))\n",
    "\n",
    "print(len(train_y_dummied.columns))\n",
    "print(len(test_y_dummied.columns))\n",
    "#print(test_X.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dimensionality Reduction TODO add indexes, column names and other columns to pca'd output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.98441663,  0.99331894,  0.99428344,  0.99516127,  0.99563725,\n",
       "        0.99602425])"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "\n",
    "#first, peel off the non-numerical variables\n",
    "\n",
    "train_X_cat = train_X[['BASKET_SIZE', 'BASKET_TYPE','BASKET_DOMINANT_MISSION','STORE_FORMAT','STORE_REGION']]\n",
    "test_X_cat = test_X[['BASKET_SIZE', 'BASKET_TYPE','BASKET_DOMINANT_MISSION','STORE_FORMAT','STORE_REGION']]\n",
    "\n",
    "train_X_num = train_X.drop(['BASKET_SIZE', 'BASKET_TYPE','BASKET_DOMINANT_MISSION','STORE_FORMAT','STORE_REGION'], axis=1)\n",
    "test_X_num = test_X.drop(['BASKET_SIZE', 'BASKET_TYPE','BASKET_DOMINANT_MISSION','STORE_FORMAT','STORE_REGION'], axis=1)\n",
    "\n",
    "\n",
    "pca = PCA(n_components = 6)\n",
    "pca.fit(train_X_num)\n",
    "\n",
    "# TODO: Transform the sample log-data using the PCA fit above\n",
    "train_X_pca= pca.transform(train_X_num)\n",
    "test_X_pca= pca.transform(test_X_num)\n",
    "\n",
    "#check cumulative sums\n",
    "display(np.cumsum(pca.explained_variance_ratio_))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Downsample the training so all strata are equal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "probably need to code this myself, based on the particular target i want"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# try the classifiers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 1.82569761  2.64490159]\n",
      " [ 2.80694332  1.41091233]\n",
      " [-2.72043225 -1.57125834]\n",
      " [-2.99372396 -0.95248164]\n",
      " [-0.85218806  3.02380223]\n",
      " [ 2.38902829 -1.33007108]\n",
      " [-2.79534903 -1.43374621]\n",
      " [-3.07394347  0.64844117]\n",
      " [-3.03515591  0.81082243]\n",
      " [ 0.46676469  3.10672418]\n",
      " [ 2.65732301  0.56229152]\n",
      " [ 2.63212356 -1.71508891]\n",
      " [ 2.28617935 -2.15475947]\n",
      " [-3.13109169 -0.25665006]\n",
      " [ 3.09796044  0.5217715 ]\n",
      " [ 1.25663563 -3.47458334]\n",
      " [-0.35081253 -3.1219441 ]\n",
      " [-1.02994699 -2.96796455]\n",
      " [-3.14002924  0.09909982]\n",
      " [-3.13944098  0.11625281]\n",
      " [ 2.37492485 -2.31010056]\n",
      " [ 0.18985749 -3.13585053]\n",
      " [ 0.26325594  3.1305432 ]\n",
      " [ 1.99838765 -2.42405676]\n",
      " [-1.72353425 -2.62660124]\n",
      " [-2.50785749  1.71543509]\n",
      " [-2.28076115 -2.16049369]\n",
      " [-0.72606345 -3.05653992]\n",
      " [-2.81317706 -1.39844172]\n",
      " [ 0.22316519  3.13365628]\n",
      " [ 2.70724173 -0.0918097 ]\n",
      " [ 1.32807457 -2.84707259]\n",
      " [-3.03309505 -0.81849789]\n",
      " [ 1.17678862  2.91286336]\n",
      " [ 2.25853354  2.18371945]\n",
      " [ 2.01695054  2.45818236]\n",
      " [ 2.58985281  1.77827637]\n",
      " [ 2.92304411  1.1512678 ]\n",
      " [-3.06159589 -0.70443951]\n",
      " [-3.13851322 -0.13906538]\n",
      " [ 1.18824651  2.36801228]\n",
      " [ 3.12888988 -0.28222774]\n",
      " [ 3.12098085 -0.35928112]\n",
      " [ 2.43656119 -1.98312227]\n",
      " [ 2.3205603  -2.11768843]\n",
      " [ 1.1638182  -2.44881272]\n",
      " [ 0.22133562 -3.13378604]\n",
      " [-3.04808459 -0.76077901]\n",
      " [ 2.63763063 -1.70660747]\n",
      " [ 1.75515004 -2.60558107]\n",
      " [-2.64087596  0.11463798]\n",
      " [ 0.5122026  -3.09955689]\n",
      " [ 1.07686656  2.95126461]\n",
      " [ 2.45325118  1.96243804]\n",
      " [ 3.12869791  0.28434805]\n",
      " [-1.84320146  2.37766879]\n",
      " [ 2.59511834 -1.77058329]\n",
      " [ 2.43021544 -1.9908936 ]\n",
      " [ 0.09436048 -3.14017523]\n",
      " [-3.13995922  0.10129411]\n",
      " [-2.98380352  1.68900173]\n",
      " [-3.11386353 -0.4164833 ]\n",
      " [-2.23958025  2.20315336]\n",
      " [-1.21847239  2.89567426]\n",
      " [ 3.00273818  0.92367084]\n",
      " [ 3.00761386  0.36743148]\n",
      " [ 1.40600124 -2.8094065 ]\n",
      " [-2.93886651 -1.11025583]\n",
      " [-1.91425776  2.49102823]\n",
      " [-1.22028626  2.89491033]\n",
      " [ 2.34221313  2.67090382]\n",
      " [ 2.17660107  2.26539449]\n",
      " [ 2.90583485  1.19403863]\n",
      " [ 2.43932864 -1.97971719]\n",
      " [ 0.25920539  3.13088118]\n",
      " [ 2.86158262  0.61796122]\n",
      " [-1.85681719  2.53413384]\n",
      " [-0.73644472  3.05405527]\n",
      " [-0.32320541  3.12492283]\n",
      " [-0.24165005  3.13228505]\n",
      " [ 2.97204757  1.29991459]\n",
      " [-1.39091189 -2.81690761]\n",
      " [-3.06777921  0.67700452]\n",
      " [-2.55850239 -1.8230935 ]\n",
      " [-0.37731344  3.11885219]\n",
      " [ 0.40108384  3.32924687]\n",
      " [ 0.71705979  3.05866469]\n",
      " [ 2.99394363  0.9517909 ]\n",
      " [-1.16970364 -2.91571566]\n",
      " [-1.86965577 -2.52467655]\n",
      " [-2.9599682   1.79497099]\n",
      " [-2.64529911  1.69469674]\n",
      " [ 0.1183837   3.13936135]\n",
      " [-1.97226951 -2.44535425]\n",
      " [ 2.56335774  1.81626031]\n",
      " [ 2.57393211 -1.36105661]\n",
      " [-1.42759716 -2.79849437]\n",
      " [-3.0094885   0.90143407]\n",
      " [-1.76209839  2.60088709]\n",
      " [-1.17357806 -2.91415836]]\n"
     ]
    }
   ],
   "source": [
    "rng = np.random.RandomState(1)\n",
    "X = np.sort(200 * rng.rand(100, 1) - 100, axis=0)\n",
    "y = np.array([np.pi * np.sin(X).ravel(), np.pi * np.cos(X).ravel()]).T\n",
    "y[::5, :] += (0.5 - rng.rand(20, 2))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('LifeStage Training: ', 1.0)\n",
      "('LifeStage Testing: ', 0.47396504139834406)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[ 348, 1022,   91],\n",
       "       [ 409, 1908,  250],\n",
       "       [ 155,  932,  320]])"
      ]
     },
     "execution_count": 134,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier #using scikitlearn 0.17.1\n",
    "rfc_ps = RandomForestClassifier(n_estimators = 100,random_state=42, criterion=\"entropy\"\n",
    "                                , max_features = 10\n",
    "                                #, max_leaf_nodes=1000\n",
    "                                #, min_samples_leaf=20\n",
    "                               )\n",
    "\n",
    "#need to handle categorical variables either through pandas.getDummies or http://scikit-learn.org/dev/modules/generated/sklearn.preprocessing.OneHotEncoder.html#sklearn.preprocessing.OneHotEncoder\n",
    "#train_X_num\n",
    "\n",
    "#print(train_X_dummied.head(10))\n",
    "#print(list(train_X_dummied.columns.values))\n",
    "\n",
    "\n",
    "\n",
    "rfc_ps = rfc_ps.fit(train_X_dummied, train_y_PS_dummied)\n",
    "rfc_ps_pred = rfc_ps.predict(test_X_dummied)\n",
    "\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "print('LifeStage Training: ', rfc_ps.score(train_X_dummied, train_y_PS_dummied))\n",
    "print('LifeStage Testing: ', rfc_ps.score(test_X_dummied, test_y_PS_dummied))\n",
    "confusion_matrix(test_y_PS_dummied, rfc_ps_pred, labels=[\"LA\", \"MM\", \"UM\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#requires sklearn 0.18\n",
    "#conda update sklearn\n",
    "\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "ann = MLPClassifier(solver='lbfgs', alpha=1e-5, hidden_layer_sizes=(5, 2), random_state=1)\n",
    "ann = ann.fit(X_train, y_train)\n",
    "ann.score(X_test,y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.67114788004136505"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "nn = KNeighborsClassifier(n_neighbors=1)\n",
    "nn = nn.fit(X_train,y_train)\n",
    "#nn_pred = nn.pred(X_test)\n",
    "nn.score(X_test,y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn import tree\n",
    "dt = tree.DecisionTreeClassifier()\n",
    "dt = dt.fit(X_train, y_train)\n",
    "dt.score(X_test,y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "ada = AdaBoostClassifier(n_estimators=100)\n",
    "ada = ada.fit(X_train, y_train)\n",
    "ada.score(X_test,y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sompy.sompy import SOMFactory #from https://github.com/sevamoo/SOMPY\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#random useful code snippets\n",
    "list(data.columns.values)\n",
    "data['SPEND'].dtype\n",
    "data_cross.head(10)"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda env:python2]",
   "language": "python",
   "name": "conda-env-python2-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
